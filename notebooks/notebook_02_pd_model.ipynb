{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ğŸ¦ Credit Risk Scorecard â€” Notebook 2: Probability of Default (PD) Model\n",
    "\n",
    "**Project:** Credit Risk Scorecard + IFRS 9 Expected Credit Loss (ECL) Engine  \n",
    "**Input:** `cs-training-clean.csv` (from Notebook 1)  \n",
    "**Objective:** Build a Logistic Regression model to estimate the probability of default (PD)\n",
    "\n",
    "**Author:** *Your Name*  \n",
    "**Date:** *2026*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Notebook Contents\n",
    "1. Load cleaned data\n",
    "2. Feature engineering & preprocessing\n",
    "3. Train / test split\n",
    "4. Logistic Regression model\n",
    "5. Model evaluation: AUC-ROC, KS Statistic, Confusion Matrix\n",
    "6. Calibration curve\n",
    "7. Feature importance\n",
    "8. Save model & predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step0",
   "metadata": {},
   "source": [
    "## Step 0: Upload Cleaned Data (Google Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in Google Colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload cs-training-clean.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Core libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ Scikit-learn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# â”€â”€ Save model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import joblib\n",
    "\n",
    "# Plot styling\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('All libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load cleaned data from Notebook 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv('cs-training-clean.csv')\n",
    "\n",
    "print(f'Data loaded: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "print(f'Default rate: {df[\"default\"].mean()*100:.2f}%')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-eng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Create additional features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Total delinquency count (combined late payment signal)\n",
    "df['total_late'] = df['late_30_59'] + df['late_60_89'] + df['late_90']\n",
    "\n",
    "# Income per dependent (financial stress indicator)\n",
    "# Add 1 to avoid division by zero\n",
    "df['income_per_dependent'] = df['monthly_income'] / (df['dependents'] + 1)\n",
    "\n",
    "# Log-transform skewed features to reduce the effect of extreme values\n",
    "df['log_income'] = np.log1p(df['monthly_income'])   # log1p = log(1 + x), handles 0 safely\n",
    "df['log_debt_ratio'] = np.log1p(df['debt_ratio'])\n",
    "\n",
    "print('New features created:')\n",
    "print('  Â· total_late          â€” sum of all late payment counts')\n",
    "print('  Â· income_per_dependent â€” monthly income divided by (dependents + 1)')\n",
    "print('  Â· log_income           â€” log-transformed monthly income')\n",
    "print('  Â· log_debt_ratio       â€” log-transformed debt ratio')\n",
    "print()\n",
    "print(f'Total features: {df.shape[1] - 1}  (excluding target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Features & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Define features and target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FEATURES = [\n",
    "    'revolving_util',\n",
    "    'age',\n",
    "    'late_30_59',\n",
    "    'late_60_89',\n",
    "    'late_90',\n",
    "    'open_credit_lines',\n",
    "    'real_estate_loans',\n",
    "    'dependents',\n",
    "    'total_late',           # engineered\n",
    "    'income_per_dependent', # engineered\n",
    "    'log_income',           # engineered\n",
    "    'log_debt_ratio',       # engineered\n",
    "]\n",
    "\n",
    "TARGET = 'default'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "# â”€â”€ Train / test split (80% train, 20% test) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set : {X_train.shape[0]:,} rows  ({y_train.mean()*100:.2f}% default rate)')\n",
    "print(f'Test set     : {X_test.shape[0]:,} rows  ({y_test.mean()*100:.2f}% default rate)')\n",
    "print()\n",
    "print('stratify=y ensures the default rate is balanced across both splits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Standardise features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Logistic Regression performs better when all features are on the same scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # fit on training data only\n",
    "X_test_scaled  = scaler.transform(X_test)         # apply same scale to test data\n",
    "\n",
    "print('Feature scaling complete (StandardScaler).')\n",
    "print('Rule: fit on TRAIN only, transform both TRAIN and TEST.')\n",
    "print('This prevents data leakage from the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## Step 4: Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# class_weight='balanced' adjusts for the imbalanced dataset (93% non-default)\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Logistic Regression model trained successfully.')\n",
    "print()\n",
    "print('Key parameters:')\n",
    "print('  Â· class_weight=\"balanced\" â€” compensates for class imbalance')\n",
    "print('  Â· max_iter=1000           â€” allows enough iterations to converge')\n",
    "print('  Â· random_state=42         â€” ensures reproducible results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# predict_proba gives probability scores (we use column 1 = P(default))\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_class = model.predict(X_test_scaled)\n",
    "\n",
    "# â”€â”€ AUC-ROC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# â”€â”€ KS Statistic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# KS = max separation between cumulative default and non-default distributions\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "ks_stat = max(tpr - fpr)\n",
    "\n",
    "print('=== Model Performance on Test Set ===')\n",
    "print(f'  AUC-ROC      : {auc:.4f}   (target: > 0.75 = good, > 0.80 = very good)')\n",
    "print(f'  KS Statistic : {ks_stat:.4f}   (target: > 0.30 = acceptable for credit scoring)')\n",
    "print()\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_class, target_names=['Non-Default', 'Default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ROC Curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='#3498db', linewidth=2.5,\n",
    "         label=f'Logistic Regression (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "# Mark KS point\n",
    "ks_idx = np.argmax(tpr - fpr)\n",
    "plt.scatter(fpr[ks_idx], tpr[ks_idx], color='#e74c3c', zorder=5, s=100,\n",
    "            label=f'KS = {ks_stat:.4f}  (threshold = {thresholds[ks_idx]:.3f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR / Recall)')\n",
    "plt.title('ROC Curve â€” Logistic Regression PD Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_06_roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: plot_06_roc_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['Non-Default', 'Default'])\n",
    "disp.plot(ax=ax, colorbar=False, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_07_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f'True Negatives  (correctly predicted Non-Default): {tn:,}')\n",
    "print(f'False Positives (Non-Default predicted as Default): {fp:,}')\n",
    "print(f'False Negatives (Default predicted as Non-Default): {fn:,}  â† costly in credit risk!')\n",
    "print(f'True Positives  (correctly predicted Default)     : {tp:,}')\n",
    "print('Saved: plot_07_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## Step 6: Calibration Curve\n",
    "\n",
    "A calibration curve checks whether the model's predicted probabilities are reliable.  \n",
    "If the model says P(default) = 20%, approximately 20% of those borrowers should actually default.  \n",
    "This is critical for IFRS 9 ECL calculations â€” we need well-calibrated PD estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Calibration curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, 's-', color='#3498db', linewidth=2,\n",
    "         markersize=8, label='Logistic Regression')\n",
    "plt.plot([0, 1], [0, 1], '--', color='grey', linewidth=1, label='Perfect Calibration')\n",
    "\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives (Actual Default Rate)')\n",
    "plt.title('Calibration Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_08_calibration_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: plot_08_calibration_curve.png')\n",
    "print()\n",
    "print('Interpretation: Points close to the diagonal = well-calibrated model.')\n",
    "print('Logistic Regression is generally well-calibrated by design.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance (Model Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Logistic Regression coefficients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Coefficient magnitude = feature importance (after standardisation)\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'coefficient': model.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=True)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "colors = ['#e74c3c' if c > 0 else '#2ecc71' for c in coef_df['coefficient']]\n",
    "plt.barh(coef_df['feature'], coef_df['coefficient'], color=colors, edgecolor='white')\n",
    "plt.axvline(x=0, color='black', linewidth=0.8)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Importance (Logistic Regression Coefficients)\\nRed = increases default risk  |  Green = decreases default risk',\n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_09_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('=== Feature Coefficients (sorted by absolute magnitude) ===')\n",
    "coef_sorted = coef_df.sort_values('coefficient', key=abs, ascending=False)\n",
    "for _, row in coef_sorted.iterrows():\n",
    "    direction = 'â†‘ risk' if row['coefficient'] > 0 else 'â†“ risk'\n",
    "    print(f\"  {direction}  {row['feature']:25s}  {row['coefficient']:+.4f}\")\n",
    "print('Saved: plot_09_feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8",
   "metadata": {},
   "source": [
    "## Step 8: Score Distribution (PD Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Distribution of predicted PD by actual class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for label, color, name in [(0, '#2ecc71', 'Non-Default'), (1, '#e74c3c', 'Default')]:\n",
    "    mask = y_test == label\n",
    "    plt.hist(y_pred_proba[mask], bins=50, alpha=0.6, color=color,\n",
    "             label=f'{name} (n={mask.sum():,})', density=True)\n",
    "\n",
    "plt.xlabel('Predicted Probability of Default (PD)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('PD Score Distribution by Actual Default Status', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plot_10_score_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: plot_10_score_distribution.png')\n",
    "print()\n",
    "print('Good separation between green and red = model discriminates well between defaults and non-defaults.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9",
   "metadata": {},
   "source": [
    "## Step 9: Save Model & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save trained model and scaler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "joblib.dump(model, 'pd_model_logistic.pkl')\n",
    "joblib.dump(scaler, 'pd_scaler.pkl')\n",
    "print('Model saved  : pd_model_logistic.pkl')\n",
    "print('Scaler saved : pd_scaler.pkl')\n",
    "\n",
    "# â”€â”€ Save test set predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = X_test.copy()\n",
    "results['actual_default'] = y_test.values\n",
    "results['predicted_pd']   = y_pred_proba\n",
    "results['predicted_class'] = y_pred_class\n",
    "results.to_csv('pd_predictions.csv', index=False)\n",
    "print('Predictions saved: pd_predictions.csv')\n",
    "\n",
    "print()\n",
    "print('=== PD Model Summary ===')\n",
    "print(f'  Model      : Logistic Regression (class_weight=balanced)')\n",
    "print(f'  Features   : {len(FEATURES)}')\n",
    "print(f'  Train rows : {X_train.shape[0]:,}')\n",
    "print(f'  Test rows  : {X_test.shape[0]:,}')\n",
    "print(f'  AUC-ROC    : {auc:.4f}')\n",
    "print(f'  KS Stat    : {ks_stat:.4f}')\n",
    "print()\n",
    "print('Next step â†’ notebook_03_ecl_engine.ipynb: IFRS 9 ECL Calculator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `pd_model_logistic.pkl` | Trained Logistic Regression model |\n",
    "| `pd_scaler.pkl` | Fitted StandardScaler |\n",
    "| `pd_predictions.csv` | Test set predictions with PD scores |\n",
    "| `plot_06_roc_curve.png` | ROC curve with AUC and KS point |\n",
    "| `plot_07_confusion_matrix.png` | Confusion matrix |\n",
    "| `plot_08_calibration_curve.png` | Calibration curve |\n",
    "| `plot_09_feature_importance.png` | Feature coefficients |\n",
    "| `plot_10_score_distribution.png` | PD score distribution by class |\n",
    "\n",
    "---\n",
    "**Next Notebook:** `notebook_03_ecl_engine.ipynb` â€” IFRS 9 ECL Calculator (ECL = PD Ã— LGD Ã— EAD Ã— Discount Factor)"
   ]
  }
 ]
}
